{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Sci Engineering Methods and Tools, midterm hints</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 28 March 2022</div>\n",
    "\n",
    "# Shakespeare and a quick intro to NLP\n",
    "The take-home is an introduction to googling and using libraries, with pandas manipulations and lots of plotting. \n",
    "\n",
    "Does NLP intrigue you? I teach an advanced class in NLP, but you need to take an ML class, first!\n",
    "\n",
    "# Text2Emotion\n",
    "```\n",
    "pip install text2emotion\n",
    "```\n",
    "\n",
    "## Fourty Winters sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import text2emotion as te\n",
    "\n",
    "forty_winters = \"\"\"\n",
    "When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty’s field,\n",
    "Thy youth’s proud livery, so gazed on now,\n",
    "Will be a tatter’d weed, of small worth held:\n",
    "Then being ask’d where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days,\n",
    "To say, within thine own deep-sunken eyes,\n",
    "Were an all-eating shame and thriftless praise.\n",
    "How much more praise deserved thy beauty’s use,\n",
    "If thou couldst answer ‘This fair child of mine\n",
    "Shall sum my count and make my old excuse,’\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel’st it cold.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Happy': 0.29, 'Angry': 0.0, 'Surprise': 0.14, 'Sad': 0.24, 'Fear': 0.33}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.get_emotion(forty_winters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern English explanation\n",
    "When forty winters have attacked your brow and wrinkled your beautiful skin, the pride and impressiveness of your youth, so much admired by everyone now, will have become a worthless, tattered weed. Then, when you are asked where your beauty’s gone and what’s happened to all the treasures you had during your youth, you will have to say only within your own eyes, now sunk deep in their sockets, where there is only a shameful confession of greed and self-obsession. How much more praise you would have deserved if you could have answered, ‘This beautiful child of mine shall give an account of my life and show that I made no misuse of my time on earth,’ proving that his beauty, because he is your son, was once yours! This child would be new-made when you are old and you would see your own blood warm when you are cold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also able to identify the emotion from the emojis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Happy': 0.0, 'Angry': 0.0, 'Surprise': 1.0, 'Sad': 0.0, 'Fear': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What an amazing day😃😃\"\n",
    "te.get_emotion(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "NLTK is arguably the #1 NLP library. It already has a built-in, pretrained sentiment analyzer called `VADER` (Valence Aware Dictionary and sEntiment Reasoner).\n",
    "\n",
    "Since VADER is pretrained, you can get results more quickly than with many other analyzers. However, VADER is best suited for language used in social media, like short sentences with some slang and abbreviations. It’s less accurate when rating longer, structured sentences, but it’s often a good launching point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a single file identifier string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3712/3354353411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#nltk.download('shakespeare')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshakespeare\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\xmldocs.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \"\"\"\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0melt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mword_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordPunctTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\xmldocs.py\u001b[0m in \u001b[0;36mxml\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mfileid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expected a single file identifier string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Read the XML in using ElementTree.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a single file identifier string"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('shakespeare')\n",
    "w = nltk.corpus.shakespeare.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import operator\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(forty_winters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll get back a dictionary of different scores. The `neg`ative, `neu`tral, and `pos`itive scores are related: They all add up to 1 and can’t be negative. The `compound` score is calculated differently. It’s not just an average, and it can range from -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little bit of fun with built-in datasets\n",
    "`.fileids()` exists in most, if not all, corpora. In the case of `movie_reviews`, each file corresponds to a single review. Note also that you’re able to filter the list of file IDs by specifying categories. This categorization is a feature specific to this corpus and others of the same type.\n",
    "\n",
    "The corpus `movie_reviews` is a collection of movie reviews included in NLTK. The special thing about this corpus is that it’s already been classified. Therefore, we can use it to judge the accuracy of the algorithms we choose when rating similar texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define `is_positive()` to work on an entire review. We’ll need to obtain that specific review using its file ID and then split it into sentences before rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rate all the reviews and see how accurate VADER is with this setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg/cv590_20712.txt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_review_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if you\\'re debating whether or not to see _breakfast_of_champions_ , ask yourself one simple question : do you want to see nick nolte in lingerie ? \\nthe only people who would get much enjoyment from alan rudolph\\'s chaotic adaptation of the kurt vonnegut novel is the cross-section of the population with the unhealthy urge to see that unpleasant sight . \\neveryone else--and i\\'m hoping that\\'s most people--would be wise to steer clear of this excrutiatingly unfunny mess . \\nactually , though , the sight of nolte in high heels is one of the more amusing things about this muddle , which focuses dwayne hoover ( bruce willis ) , the owner of dwayne hoover\\'s exit 11 motor village in midland city . \\nnot only is he a huge success as a businessman , he\\'s also something of a celebrity , his face made recognizable by an ongoing series of television commercials . \\nwith a nice home and family to boot , dwayne appears to have it all the ingredients to be happy--yet he\\'s not . \\nhis wife celia ( barbara hershey ) is perpetually in a pill-induced haze ; his son george ( lukas haas ) is a flamboyant lounge singer who goes by the stage name \" bunny . \" \\nnot only that , the environmental protection agency is on dwayne\\'s ass over a building development project . \\nit\\'s enough to send dwayne on a nervous breakdown--that is , if he doesn\\'t succeed in blowing his brains out first . \\nmeanwhile , midland city is about to host a fine arts festival , and the guest of honor is one kilgore trout ( albert finney ) , a writer who is far from the renowned author the festival\\'s organizer ( buck henry ) was led to believe--in fact , he\\'s a penniless hack who writes second-rate sci-fi that appears in porn magazines . \\nhis trek to midland city is also a spiritual journey , one that reaches its apex after meeting dwayne , who for some reason thinks that trout will hold for him all of life\\'s answers . \\nthe above is already a longer plot synopsis than i usually give in my reviews , but , ironically , i have barely scratched the surface . \\ni haven\\'t yet mentioned wayne hoobler ( omar epps ) , an ex-con with an obsessive admiration for the similarly-named dwayne . \\nthen there\\'s the matter of francine ( glenne headly ) , dwayne\\'s devoted secretary . \\nnot to mention dwayne\\'s employee and old friend harry lesabre ( nolte ) , the one with the secret penchant for cross-dressing . \\nand so on . \\nthe film is essentially dwayne\\'s story , but too often rudolph goes on distracting tangents with the eccentric peripheral players that one often wonders what the point is . \\nrudolph does arrive at a point ( more on that later ) , but it\\'s blunted and obscured by his hyperactive approach to the material . \\nthe surreal visual style , complete with printed words flying through the air and into dwayne\\'s ears , is obviously meant to convey a sense of madness , but its bludgeoning nature is likely to make viewers mad . \\nthe actors are called on to act accordingly , resulting in some of the worst , most overdone work all of them have ever turned in . \\nwillis fares best of all--but that\\'s because his frozen expression of befuddled bewilderment mirrors that of the audience . \\nwith such an aggressively outrageous atmosphere for nearly all of its running time , it comes as a shock when things suddenly turn serious , and rudolph tries to make a statement . \\nunlike _american_beauty_ ( a film that _breakfast_ resembles in more than a few ways , to its great detriment ) , there isn\\'t any palpably earnest undercurrent that would prepare the audience for the big shift . \\nas such , the cartoony characters fail to win a sympathy that needs to be earned ; and the film attempts , to no avail , to reach a profundity that it doesn\\'t deserve . \\nvonnegut\\'s original novel is considered a classic , but it had been called unfilmable--the same that was said of hunter s . thompson\\'s _fear_and_loathing_in_las_vegas_ , which was disastrously committed to film last year by terry gilliam . \\nwith the similar failure of _breakfast_of_champions_ , will hollywood ever learn that books labeled \" unfilmable \" inevitably results in a film that is unwatchable ? \\nlikely not . \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.movie_reviews.raw(all_review_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.00% correct\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "    if is_positive(review_id):\n",
    "        if review_id in positive_review_ids:\n",
    "            correct += 1\n",
    "    else:\n",
    "         if review_id in negative_review_ids:\n",
    "            correct += 1\n",
    "\n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rating all reviews, we can see that only 64% were correctly classified by VADER using the logic defined in `is_positive()`.\n",
    "\n",
    "A 64% accuracy rating isn’t great, but it’s a start.\n",
    "\n",
    "In order to train and evaluate an improved classifier, we’ll need to build a list of features for each text we’ll analyze.\n",
    "\n",
    "By using the predefined categories in the `movie_reviews` corpus, let's create sets of positive and negative words, then determine which ones occur most frequently across each set. Let's begin by excluding unwanted words and building the initial category groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Dino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to create frequency distributions for our custom feature. Since some words are present in both positive and negative sets, let's begin by finding the common set so we can remove it from the distribution objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soft',\n",
       " 'morosely',\n",
       " 'extravaganzas',\n",
       " 'sense',\n",
       " 'trick',\n",
       " 'chased',\n",
       " 'opening',\n",
       " 'betrays',\n",
       " 'old',\n",
       " 'wiser']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(common_set, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hanks',\n",
       " 'vividly',\n",
       " 'kimble',\n",
       " 'profile',\n",
       " 'attentive',\n",
       " 'soviet',\n",
       " 'ghost',\n",
       " 'spacey',\n",
       " 'societal',\n",
       " 'fa']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(top_100_positive, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manchurian',\n",
       " 'putrid',\n",
       " 'flubber',\n",
       " 'snipes',\n",
       " 'tediously',\n",
       " 'abysmal',\n",
       " 'consecutive',\n",
       " 'weighed',\n",
       " 'droppingly',\n",
       " 'fetch']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(top_100_negative, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we’re left with unique positive and negative words in each frequency distribution object, we can finally build sets from the most common words in each distribution. \n",
    "\n",
    "The amount of words in each set is something we could tweak in order to determine its effect on sentiment analysis.\n",
    "\n",
    "With our new feature set ready to use, the first prerequisite for training a classifier is to define a function that will extract features from a given piece of data.\n",
    "\n",
    "Since we’re looking for positive movie reviews, let's focus on the features that indicate positivity, including VADER scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_neg_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    negative_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_negative:\n",
    "                wordcount += 1\n",
    "        negative_scores.append(sia.polarity_scores(sentence)[\"neg\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_negative\"] = mean(negative_scores)\n",
    "    features[\"wordcount2\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_pos_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_neg_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the classifier involves splitting the feature set so that one portion can be used for training and the other for evaluation, then calling `.train()`.\n",
    "\n",
    "We can use `classifier.show_most_informative_features()` to determine which features are most indicative of a specific property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n"
     ]
    }
   ],
   "source": [
    "# Use 1/4 of the set for training\n",
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9886666666666667"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, features[train_count:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding features has dramatically improved VADER’s initial accuracy, from 64% to 99%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Additional Classifiers\n",
    "NLTK provides a class that can use most classifiers from the popular machine learning framework `scikit-learn`.\n",
    "\n",
    "Many of the classifiers that `scikit-learn` provides can be instantiated quickly since they have defaults that often work well. Let's integrate them within NLTK to classify text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these classifiers imported, we’ll first have to instantiate each one. Thankfully, all of these have pretty good defaults and don’t require much tweaking.\n",
    "\n",
    "To aid in accuracy evaluation, it’s helpful to have a mapping of classifier names and their instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these instances for training and accuracy evaluation.\n",
    "\n",
    "Since NLTK allows us to integrate scikit-learn classifiers directly into its own classifier class, the training and classification processes will use the same methods we’ve already seen, `.train()` and `.classify()`.\n",
    "\n",
    "We’ll also be able to leverage the same features list we built earlier by means of `extract_features()`. To jog our memory, here’s how we built the features list:\n",
    "```\n",
    "features = [\n",
    "    (extract_pos_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_neg_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])\n",
    "```\n",
    "\n",
    "The features list contains tuples whose first item is a set of features given by extract_features(), and whose second item is the classification label from preclassified data in the movie_reviews corpus.\n",
    "\n",
    "Since the first half of the list contains only positive reviews, begin by shuffling it, then iterate over all classifiers to train and evaluate each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.60% - BernoulliNB\n",
      "66.60% - ComplementNB\n",
      "66.60% - MultinomialNB\n",
      "69.00% - KNeighborsClassifier\n",
      "64.07% - DecisionTreeClassifier\n",
      "68.67% - RandomForestClassifier\n",
      "71.33% - LogisticRegression\n",
      "72.73% - MLPClassifier\n",
      "71.27% - AdaBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "# Use 1/4 of the set for training\n",
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "    classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "    classifier.train(features[:train_count])\n",
    "    accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
    "    print(F\"{accuracy:.2%} - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this doesn’t mean that the `MLPClassifier` will continue to be the best one as you engineer new features, having additional classification algorithms at our disposal is clearly advantageous.\n",
    "\n",
    "NTLK that allow us to process text into objects that we can filter and manipulate, which allows us to analyze text data to gain information about its properties. We can also use different classifiers to perform sentiment analysis on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "Text Blob is a simple python library used to perform NLP task like tokenization, Noun phrase extraction, POS-Tagging, Words inflection and lemmatization, N-grams, Sentiment Analysis.\n",
    "```\n",
    "pip install textblob\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.22491258741258743, subjectivity=0.5234265734265734)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "TextBlob(forty_winters).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sentiment', 'NN'),\n",
       " ('analysis', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('interpretation', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('classification', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('emotions', 'NNS'),\n",
       " ('positive', 'JJ'),\n",
       " ('negative', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('neutral', 'JJ'),\n",
       " ('within', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('using', 'VBG'),\n",
       " ('text', 'JJ'),\n",
       " ('analysis', 'NN'),\n",
       " ('techniques', 'NNS'),\n",
       " ('Sentiment', 'NN'),\n",
       " ('analysis', 'NN'),\n",
       " ('allows', 'VBZ'),\n",
       " ('businesses', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('identify', 'VB'),\n",
       " ('customer', 'NN'),\n",
       " ('sentiment', 'NN'),\n",
       " ('toward', 'IN'),\n",
       " ('products', 'NNS'),\n",
       " ('brands', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('services', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('online', 'JJ'),\n",
       " ('conversations', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('feedback', 'NN')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob   \n",
    "text = '''                                       \n",
    "Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text \n",
    "data using text analysis techniques. Sentiment analysis allows \n",
    "businesses to identify customer sentiment toward products, brands or services in online conversations and feedback.\n",
    "'''\n",
    "blob = TextBlob(text) \n",
    "blob.tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"这真的很好！\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = TextBlob(\"This is really good !\") \n",
    "sentence.translate(to=\"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"यह सचमुच अच्छा है !\")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.translate(to=\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Αυτό είναι πραγματικά καλό!\")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.translate(to=\"el\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair\n",
    "Another NLP library, [Flair](https://github.com/flairNLP/flair) allows us to apply our state-of-the-art natural language processing (NLP) models to our text, such as named entity recognition (NER), part-of-speech tagging (PoS), special support for biomedical data, sense disambiguation and classification, with support for a rapidly growing number of languages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
